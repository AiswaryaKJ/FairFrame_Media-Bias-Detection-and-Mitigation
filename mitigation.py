# -*- coding: utf-8 -*-
"""mitigation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W9kIuUyFjlkFHCqJzSnveD_5fwja_eCU
"""

!pip install transformers datasets sentencepiece -q

import pandas as pd
import torch
from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration
from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

# Verify the path using the "Files" tab in Colab and copy the path
file_path = "/content/drive/MyDrive/FAIRFRAME/dataset_10000.csv"  # Replace with the copied path
df = pd.read_csv(file_path)

# Display sample data
print("Sample Data:")
print(df.head())

# Check for missing values before dropping
print("Missing values before dropping:")
print(df.isnull().sum())

# Remove null values
df.dropna(inplace=True)

# Check if DataFrame is empty after dropping null values
if df.empty:
    print("DataFrame is empty after dropping null values. Please check your data.")
else:
    # Train-validation split (80%-20%)
    train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['detoxified'], test_size=0.2, random_state=42)

    # Convert to Huggingface Dataset
    train_dataset = Dataset.from_dict({'text': train_texts.tolist(), 'unbiased': train_labels.tolist()})
    val_dataset = Dataset.from_dict({'text': val_texts.tolist(), 'unbiased': val_labels.tolist()})

# prompt: Dataset({
#     features: ['text', 'unbiased'],
#     num_rows: 1893
# })
# need only the text

# Assuming you have the train_dataset and val_dataset defined as in your provided code

# Extract only the 'text' column from the train_dataset


# Extract only the 'text' column from the val_dataset
val_texts_only = [example['text'] for example in val_dataset]

# Now you have two lists containing only the 'text' data from your train and validation datasets

print( val_texts_only[1],"\n")

# prompt: need shape oh val_dataset

val_dataset.text[:5]

max_input_length = 128
max_target_length = 128

def preprocess_data(data, tokenizer):
    inputs = ["unbias: " + i for i in data['text']]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(data['unbiased'], max_length=max_target_length, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# T5 Model
t5_tokenizer = T5Tokenizer.from_pretrained("t5-large")
t5_model = T5ForConditionalGeneration.from_pretrained("t5-large")

# BART Model
'''bart_tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
bart_model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")

# Pegasus Model
pegasus_tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-large")
pegasus_model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
'''

train_dataset_t5 = train_dataset.map(lambda x: preprocess_data(x, t5_tokenizer), batched=True, remove_columns=['text', 'unbiased'])
val_dataset_t5 = val_dataset.map(lambda x: preprocess_data(x, t5_tokenizer), batched=True, remove_columns=['text', 'unbiased'])

'''train_dataset_bart = train_dataset.map(lambda x: preprocess_data(x, bart_tokenizer), batched=True, remove_columns=['text', 'unbiased'])
val_dataset_bart = val_dataset.map(lambda x: preprocess_data(x, bart_tokenizer), batched=True, remove_columns=['text', 'unbiased'])

train_dataset_pegasus = train_dataset.map(lambda x: preprocess_data(x, pegasus_tokenizer), batched=True, remove_columns=['text', 'unbiased'])
val_dataset_pegasus = val_dataset.map(lambda x: preprocess_data(x, pegasus_tokenizer), batched=True, remove_columns=['text', 'unbiased'])
'''

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/FAIRFRAME",
    evaluation_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=3,
    num_train_epochs=3,
    weight_decay=0.05,
    save_total_limit=1,
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)

def train_model(model, tokenizer, train_dataset, val_dataset):
    train_losses = []  # Initialize empty list for train_losses
    val_losses = []  # Initialize empty list for val_losses
    epochs = [] # Initialize empty list for epochs

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    train_results = trainer.train() # capturing train result to access metrics
    return model


# Train T5
print("\nTraining T5...")
t5_model = train_model(t5_model, t5_tokenizer, train_dataset_t5, val_dataset_t5)

#get training loss values
#loss_values=train_results_t5.metrics['train_loss']
#epochs=range(1,len(loss_values)+1)

''' #Train BART
print("\nTraining BART...")
bart_model = train_model(bart_model, bart_tokenizer, train_dataset_bart, val_dataset_bart)

# Train Pegasus
print("\nTraining Pegasus...")
pegasus_model = train_model(pegasus_model, pegasus_tokenizer, train_dataset_pegasus, val_dataset_pegasus)'''

save_path="/content/drive/MyDrive/FAIRFRAME/mitigation_model"
t5_model.save_pretrained(save_path)
t5_tokenizer.save_pretrained(save_path)

from transformers import T5Tokenizer, T5ForConditionalGeneration
model_path="/content/drive/MyDrive/FAIRFRAME/mitigation_model"
mitigation_tokenizer = T5Tokenizer.from_pretrained(model_path)
mitigation_model = T5ForConditionalGeneration.from_pretrained(model_path)

max_input_length = 128
max_target_length = 128

def generate_unbiased_text(text):
    """Generates unbiased text using the T5 model."""
    input_ids = mitigation_tokenizer("Unbias: " + text, return_tensors="pt", padding='max_length', max_length=max_input_length, truncation=True).input_ids
    output_ids = mitigation_model.generate(input_ids, max_length=max_target_length, num_beams=4, early_stopping=True)
    return mitigation_tokenizer.decode(output_ids[0], skip_special_tokens=True)
user_input = input("Enter text: ")
print("Unbiased:", generate_unbiased_text(user_input))

!pip install transformers torch pandas scikit-learn

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/FAIRFRAME/only biasscore.csv')  # Replace with your actual file name
texts = data['text'].tolist()  # Adjust the column name as necessary
bias_scores = data['bias_score'].tolist()  # Adjust the column name as necessary

print(data.head())  # Display the first few rows of the dataset

from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(texts, bias_scores, test_size=0.2, random_state=42)

print(f'Training samples: {len(X_train)}, Testing samples: {len(X_test)}')

from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class BiasDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = tokenizer(self.texts[idx], padding='max_length', truncation=True, return_tensors='pt', max_length=128)
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float).unsqueeze(0)  # For regression
        }

# Create DataLoaders
train_dataset = BiasDataset(X_train, y_train)
test_dataset = BiasDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch
from transformers import BertForSequenceClassification, AdamW

# Initialize the model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # 1 for regression
optimizer = AdamW(model.parameters(), lr=3e-5)

# Check if a GPU is available and move the model to the GPU if possible
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

model.train()
for epoch in range(14):  # You can increase the number of epochs
    for batch in train_loader:
        optimizer.zero_grad()

        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')

from sklearn.metrics import mean_squared_error

model.eval()
predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = outputs.logits.squeeze().tolist()  # Get predictions

        if not isinstance(preds, list):
            preds = [preds]
        predictions.extend(preds)

# Calculate the Mean Squared Error
mse = mean_squared_error(y_test, predictions)
print(f'Mean Squared Error: {mse}')

path="/content/drive/MyDrive/FAIRFRAME/bias_score_model"
model.save_pretrained(path)
tokenizer.save_pretrained(path)

import torch
print(torch.__version__)
print(torch.cuda.is_available())  # Check if GPU is available

import torch
from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration


# Load the Bias Detection Model
bias_tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model")
bias_model = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model", num_labels=1)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bias_model.to(device)

# Load the Mitigation Model (T5)
mitigation_tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model.to(device)

# Constants
max_input_length = 128
max_target_length = 128

def detect_bias(text):
    """Detects bias in the input text using the BERT model."""
    encoding = bias_tokenizer(text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = bias_model(input_ids=input_ids, attention_mask=attention_mask)
        bias_score = outputs.logits.squeeze().item()

    return bias_score

def generate_unbiased_text(text):
    """Generates unbiased text using the T5 model."""
    input_ids = mitigation_tokenizer("unbias: " + text, return_tensors="pt", padding='max_length', max_length=max_input_length, truncation=True).input_ids
    input_ids = input_ids.to(device)

    with torch.no_grad():
        output = mitigation_model.generate(input_ids, max_length=max_target_length, num_beams=4, early_stopping=True)

    return mitigation_tokenizer.decode(output[0], skip_special_tokens=True)

# Get user input
user_input = input("Enter text: ")

# Detect bias
bias_score = detect_bias(user_input)
print("\nOriginal:", user_input)
#print("Bias Score:", bias_score)
# Check bias score and process accordingly
if bias_score > 3:
    intertext=generate_unbiased_text(user_input)
    new_score=detect_bias(intertext)
    i=0
    while(new_score>3 and i<=3):
        i+=1
        intertext=generate_unbiased_text(intertext)
        new_score=detect_bias(intertext)
        print("iteration:",i,"text:",intertext,"score:",new_score)
    print("unbiased text:",intertext)#,"\n new bias score",new_score''')
else:
    print("Low bias, no need to unbias.")

import torch
from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration


# Load the Bias Detection Model
bias_tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model")
bias_model = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model", num_labels=1)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bias_model.to(device)

# Load the Mitigation Model (T5)
mitigation_tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model.to(device)

# Constants
max_input_length = 128
max_target_length = 128

def detect_bias(text):
    """Detects bias in the input text using the BERT model."""
    encoding = bias_tokenizer(text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = bias_model(input_ids=input_ids, attention_mask=attention_mask)
        bias_score = outputs.logits.squeeze().item()

    return bias_score

def generate_unbiased_text(text):
    """Generates unbiased text using the T5 model."""
    input_ids = mitigation_tokenizer("unbias: " + text, return_tensors="pt", padding='max_length', max_length=max_input_length, truncation=True).input_ids
    input_ids = input_ids.to(device)

    with torch.no_grad():
        output = mitigation_model.generate(input_ids, max_length=max_target_length, num_beams=4, early_stopping=True)

    return mitigation_tokenizer.decode(output[0], skip_special_tokens=True)

# Get user input
user_input = input("Enter text: ")

# Detect bias
bias_score = detect_bias(user_input)
print("\nOriginal:", user_input)
#print("Bias Score:", bias_score)
# Check bias score and process accordingly
if bias_score > 3:
    intertext=generate_unbiased_text(user_input)
    new_score=detect_bias(intertext)
    i=0
    while(new_score>3):
        i+=1
        intertext=generate_unbiased_text(intertext)
        new_score=detect_bias(intertext)
        print("iteration:",i,"text:",intertext,"score:",new_score)
    print("unbiased text:",intertext)#,"\n new bias score",new_score''')
else:
    print("Low bias, no need to unbias.")

import torch
from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration


# Load the Bias Detection Model
bias_tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model")
bias_model = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/FAIRFRAME/bias_score_model", num_labels=1)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bias_model.to(device)

# Load the Mitigation Model (T5)
mitigation_tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/FAIRFRAME/mitigation_model")
mitigation_model.to(device)

# Constants
max_input_length = 128
max_target_length = 128

def detect_bias(text):
    """Detects bias in the input text using the BERT model."""
    encoding = bias_tokenizer(text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = bias_model(input_ids=input_ids, attention_mask=attention_mask)
        bias_score = outputs.logits.squeeze().item()

    return bias_score

def generate_unbiased_text(text):
    """Generates unbiased text using the T5 model."""
    input_ids = mitigation_tokenizer("unbias: " + text, return_tensors="pt", padding='max_length', max_length=max_input_length, truncation=True).input_ids
    input_ids = input_ids.to(device)

    with torch.no_grad():
        output = mitigation_model.generate(input_ids, max_length=max_target_length, num_beams=4, early_stopping=True)

    return mitigation_tokenizer.decode(output[0], skip_special_tokens=True)

import ipywidgets as widgets
from IPython.display import display

# Create a large text area
text_area = widgets.Textarea(
    placeholder="Enter your text here...",
    layout=widgets.Layout(width="100%", height="200px")  # Adjust size
)

display(text_area)

# Button to get input
button = widgets.Button(description="Submit")

output = widgets.Output()

def on_button_click(b):
    with output:
        output.clear_output()
        user_input = text_area.value
        #print("\nUser Input:", user_input)

        # Detect bias
        bias_score = detect_bias(user_input)
        print("\nOriginal:", user_input)
        print("\nBias Score:", bias_score)
        # Check bias score and process accordingly
        if bias_score > 3:
            intertext=generate_unbiased_text(user_input)
            new_score=detect_bias(intertext)
            i=0
            while(new_score>3 and i<=3):
                i+=1
                intertext=generate_unbiased_text(intertext)
                new_score=detect_bias(intertext)
                #print("iteration:",i,"text:",intertext,"score:",new_score)
            print("\nunbiased text:",intertext)#,"\n new bias score",new_score''')
        else:
            print("Low bias, no need to unbias.")

button.on_click(on_button_click)

display(button, output)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def compute_similarity(text1, text2):
    # Create a TF-IDF vectorizer
    vectorizer = TfidfVectorizer()

    # Fit and transform the input texts
    tfidf_matrix = vectorizer.fit_transform([text1, text2])

    # Compute cosine similarity
    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])

    return similarity_score[0][0]

# Example input
if bias_score>3:
  text1 = user_input
  text2 = intertext
  similarity = compute_similarity(text1, text2)
  print(f"Cosine Similarity Score: {similarity:.4f}")
else:
  print("no need to compare")

# Compute similarity

pip install transformers torch rouge-score nltk textstat



import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import textstat
import numpy as np

# Load fine-tuned T5 model
model_name = "/content/drive/MyDrive/FAIRFRAME/mitigation_model"  # Update path if needed
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Sample test dataset (replace with actual dataset)
test_set = val_texts_only  # Ensure `val_texts_only` is defined

# Function to generate mitigated text using T5
def generate_mitigated_text(text):
    if not text.strip():
        return ""  # Handle empty inputs
    input_ids = tokenizer(text, return_tensors="pt", padding=True, truncation=True).input_ids.to(device)
    output_ids = model.generate(input_ids, max_new_tokens=100)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Function to calculate ROUGE scores
def calculate_rouge(reference, generated):
    scorer = rouge_scorer.RougeScorer(['rouge-1', 'rouge-2', 'rouge-l'], use_stemmer=True)
    return scorer.score(reference, generated)

# Function to calculate BLEU score with smoothing
def calculate_bleu(reference, generated):
    reference_tokens = reference.split()
    generated_tokens = generated.split()
    if not reference_tokens or not generated_tokens:
        return 0  # Avoid issues with empty text
    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)

# Function to calculate Flesch Reading Ease Score
def calculate_flesch_score(text):
    return textstat.flesch_reading_ease(text) if text.strip() else 0

# Lists to store scores
rouge_1_scores, rouge_2_scores, rouge_l_scores = [], [], []
bleu_scores = []
flesch_original_scores, flesch_mitigated_scores = [], []

# Process test set
for original_text in test_set:
    mitigated_text = generate_mitigated_text(original_text)

    rouge_scores = calculate_rouge(original_text, mitigated_text)
    rouge_1_scores.append(rouge_scores['rouge-1'].fmeasure)
    rouge_2_scores.append(rouge_scores['rouge-2'].fmeasure)
    rouge_l_scores.append(rouge_scores['rouge-l'].fmeasure)

    bleu_scores.append(calculate_bleu(original_text, mitigated_text))

    flesch_original_scores.append(calculate_flesch_score(original_text))
    flesch_mitigated_scores.append(calculate_flesch_score(mitigated_text))

# Compute average scores
average_rouge_1 = np.mean(rouge_1_scores)
average_rouge_2 = np.mean(rouge_2_scores)
average_rouge_l = np.mean(rouge_l_scores)
average_bleu = np.mean(bleu_scores)
average_flesch_original = np.mean(flesch_original_scores)
average_flesch_mitigated = np.mean(flesch_mitigated_scores)

# Print results
print(f"Average ROUGE-1 Score: {average_rouge_1:.4f}")
print(f"Average ROUGE-2 Score: {average_rouge_2:.4f}")
print(f"Average ROUGE-L Score: {average_rouge_l:.4f}")
print(f"Average BLEU Score: {average_bleu:.4f}")
print(f"Average Flesch Reading Ease Score (Original): {average_flesch_original:.2f}")
print(f"Average Flesch Reading Ease Score (Mitigated): {average_flesch_mitigated:.2f}")

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import textstat
import numpy as np

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import textstat
import numpy as np

def print_debug_info(test_set):
    print("Debug Information:")
    print(f"Number of texts in test set: {len(test_set)}")
    if test_set:
        print("First few texts:")
        for i, text in enumerate(test_set[:3], 1):
            print(f"{i}. Text length: {len(text)} characters")
            print(f"   Preview: {text[:100]}...")

def evaluate_text_mitigation(model, tokenizer, test_set, device):
    # Print debug info first
    print_debug_info(test_set)

    # Validation checks
    if not test_set:
        print("Error: Test set is empty!")
        return None

    # Prepare lists to store results
    rouge_1_scores, rouge_2_scores, rouge_l_scores = [], [], []
    bleu_scores = []
    flesch_original_scores, flesch_mitigated_scores = [], []

    # Scorer for ROUGE
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Process each text
    for idx, original_text in enumerate(test_set, 1):
        try:
            # Ensure text is not empty
            if not original_text or not original_text.strip():
                print(f"Skipping empty text at index {idx}")
                continue

            # Tokenize and generate
            inputs = tokenizer(
                original_text,
                max_length=512,
                return_tensors="pt",
                truncation=True,
                padding=True
            ).to(device)

            # Generate mitigated text
            outputs = model.generate(
                inputs.input_ids,
                max_length=100,
                num_return_sequences=1
            )
            mitigated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Calculate ROUGE scores
            rouge_scores = scorer.score(original_text, mitigated_text)
            rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)
            rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)
            rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)

            # Calculate BLEU score
            reference_tokens = original_text.split()
            generated_tokens = mitigated_text.split()
            bleu_score = sentence_bleu(
                [reference_tokens],
                generated_tokens,
                smoothing_function=SmoothingFunction().method1
            )
            bleu_scores.append(bleu_score)

            # Calculate Flesch scores
            flesch_original_scores.append(textstat.flesch_reading_ease(original_text) or 0)
            flesch_mitigated_scores.append(textstat.flesch_reading_ease(mitigated_text) or 0)

            # Print progress
            if idx % 10 == 0:
                print(f"Processed {idx} texts...")

        except Exception as e:
            print(f"Error processing text at index {idx}: {e}")

    # Compute and print results
    print("\nEvaluation Results:")
    print(f"ROUGE-1: {np.mean(rouge_1_scores):.4f}")
    print(f"ROUGE-2: {np.mean(rouge_2_scores):.4f}")
    print(f"ROUGE-L: {np.mean(rouge_l_scores):.4f}")
    print(f"BLEU Score: {np.mean(bleu_scores):.4f}")
    print(f"Flesch (Original): {np.mean(flesch_original_scores):.2f}")
    print(f"Flesch (Mitigated): {np.mean(flesch_mitigated_scores):.2f}")

    return {
        'rouge1': np.mean(rouge_1_scores),
        'rouge2': np.mean(rouge_2_scores),
        'rougeL': np.mean(rouge_l_scores),
        'bleu': np.mean(bleu_scores),
        'flesch_original': np.mean(flesch_original_scores),
        'flesch_mitigated': np.mean(flesch_mitigated_scores)
    }

# Actual usage
try:
    # Ensure these variables are defined before running
    model_name = "/content/drive/MyDrive/FAIRFRAME/mitigation_model"

    # Load model and tokenizer
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Ensure test_set is defined
    if 'val_texts_only' not in locals():
        print("Error: val_texts_only is not defined. Please define your test set.")
    else:
        # Run evaluation
        results = evaluate_text_mitigation(model, tokenizer, val_texts_only, device)
except Exception as e:
    print(f"Unexpected error: {e}")

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import numpy as np

# Batch size for processing
BATCH_SIZE = 16

def print_debug_info(test_set):
    print("Debug Information:")
    print(f"Total texts in test set: {len(test_set)}")
    if test_set:
        print("First few texts:")
        for i, text in enumerate(test_set[:3], 1):
            print(f"{i}. Text length: {len(text)} characters")
            print(f"   Preview: {text[:100]}...")

def batch_evaluate(model, tokenizer, test_set, device, batch_size=BATCH_SIZE):
    print_debug_info(test_set)

    if not test_set:
        print("Error: Test set is empty!")
        return None

    rouge_1_scores, rouge_2_scores, rouge_l_scores = [], [], []
    bleu_scores = []
    flesch_original_scores, flesch_mitigated_scores = [], []

    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Process in batches
    for i in range(0, len(test_set), batch_size):
        batch_texts = test_set[i:i + batch_size]

        try:
            # Ensure batch is not empty
            batch_texts = [text.strip() for text in batch_texts if text.strip()]
            if not batch_texts:
                continue

            # Tokenize and generate mitigated texts as batch
            inputs = tokenizer(batch_texts, max_length=512, return_tensors="pt", truncation=True, padding=True).to(device)
            outputs = model.generate(inputs.input_ids, max_length=100, num_return_sequences=1)
            mitigated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]

            # Calculate Scores for each text in the batch
            for original_text, mitigated_text in zip(batch_texts, mitigated_texts):
                rouge_scores = scorer.score(original_text, mitigated_text)
                rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)
                rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)
                rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)

                reference_tokens = original_text.split()
                generated_tokens = mitigated_text.split()
                bleu_scores.append(sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1))

                flesch_original_scores.append(textstat.flesch_reading_ease(original_text) or 0)
                flesch_mitigated_scores.append(textstat.flesch_reading_ease(mitigated_text) or 0)

            # Display batch-wise results
            print(f"\nProcessed Batch {i//batch_size + 1}:")
            print(f"  Average ROUGE-1: {np.mean(rouge_1_scores):.4f}")
            print(f"  Average ROUGE-2: {np.mean(rouge_2_scores):.4f}")
            print(f"  Average ROUGE-L: {np.mean(rouge_l_scores):.4f}")
            print(f"  Average BLEU Score: {np.mean(bleu_scores):.4f}")
            print(f"  Flesch (Original): {np.mean(flesch_original_scores):.2f}")
            print(f"  Flesch (Mitigated): {np.mean(flesch_mitigated_scores):.2f}")

        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")

    # Compute final results
    print("\nFinal Evaluation Results:")
    print(f"Overall ROUGE-1: {np.mean(rouge_1_scores):.4f}")
    print(f"Overall ROUGE-2: {np.mean(rouge_2_scores):.4f}")
    print(f"Overall ROUGE-L: {np.mean(rouge_l_scores):.4f}")
    print(f"Overall BLEU Score: {np.mean(bleu_scores):.4f}")
    print(f"Overall Flesch (Original): {np.mean(flesch_original_scores):.2f}")
    print(f"Overall Flesch (Mitigated): {np.mean(flesch_mitigated_scores):.2f}")

    return {
        'rouge1': np.mean(rouge_1_scores),
        'rouge2': np.mean(rouge_2_scores),
        'rougeL': np.mean(rouge_l_scores),
        'bleu': np.mean(bleu_scores),
        'flesch_original': np.mean(flesch_original_scores),
        'flesch_mitigated': np.mean(flesch_mitigated_scores)
    }

# Actual execution
try:
    model_name = "/content/drive/MyDrive/FAIRFRAME/mitigation_model"

    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    if 'val_texts_only' not in locals():
        print("Error: val_texts_only is not defined. Please define your test set.")
    else:
        results = batch_evaluate(model, tokenizer, val_texts_only, device)
except Exception as e:
    print(f"Unexpected error: {e}")

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer, util
import numpy as np

# Load a sentence embedding model for cosine similarity
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Batch size for processing
BATCH_SIZE = 16

def batch_evaluate_cosine(model, tokenizer, test_set, device, batch_size=BATCH_SIZE):
    if not test_set:
        print("Error: Test set is empty!")
        return None

    cosine_similarities = []

    # Process in batches
    for i in range(0, len(test_set), batch_size):
        batch_texts = test_set[i:i + batch_size]

        try:
            batch_texts = [text.strip() for text in batch_texts if text.strip()]
            if not batch_texts:
                continue

            # Tokenize and generate mitigated texts as batch
            inputs = tokenizer(batch_texts, max_length=512, return_tensors="pt", truncation=True, padding=True).to(device)
            outputs = model.generate(inputs.input_ids, max_length=100, num_return_sequences=1)
            mitigated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]

            # Compute embeddings for cosine similarity
            original_embeddings = embedding_model.encode(batch_texts, convert_to_tensor=True)
            mitigated_embeddings = embedding_model.encode(mitigated_texts, convert_to_tensor=True)
            batch_cosine_similarities = util.paired_cosine_sim(original_embeddings, mitigated_embeddings).cpu().numpy()

            cosine_similarities.extend(batch_cosine_similarities)

            # Display batch-wise results
            print(f"\nBatch {i//batch_size + 1}:")
            print(f"  Average Cosine Similarity: {np.mean(batch_cosine_similarities):.4f}")

        except Exception as e:
            print(f"Error processing batch {i//batch_size + 1}: {e}")

    # Compute final cosine similarity result
    overall_cosine_similarity = np.mean(cosine_similarities)
    print(f"\nOverall Cosine Similarity: {overall_cosine_similarity:.4f}")

    return overall_cosine_similarity

# Actual execution
try:
    model_name = "/content/drive/MyDrive/FAIRFRAME/mitigation_model"

    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    if 'val_texts_only' not in locals():
        print("Error: val_texts_only is not defined. Please define your test set.")
    else:
        cosine_similarity_score = batch_evaluate_cosine(model, tokenizer, val_texts_only, device)
except Exception as e:
    print(f"Unexpected error: {e}")